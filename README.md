Got it â€” I will rewrite the **entire README** again from scratch,
**matching Jayâ€™s repository structure exactly**,
**but updated to reflect YOUR project**, including:

âœ” only **AAPL, GOOGL, AMZN** streamed
âœ” same folder structure as Jay
âœ” clean, professional format
âœ” ready to paste into GitHub

Here is your final polished README.md:

---

# ğŸ“ˆ Real-Time Stock Market Analytics Pipeline â€” Modern Data Stack Project

This project showcases a complete **real-time data engineering pipeline** using the **Modern Data Stack (MDS)**.
It streams live stock market data, lands it in object storage, orchestrates ingestion into Snowflake, transforms it using dbt, and visualizes insights in Power BI â€” all fully automated and containerized.

---

## ğŸ—ï¸ Architecture Overview

```
Finnhub API 
   â†’ Kafka Producer 
   â†’ Kafka Broker 
   â†’ Kafka Consumer 
   â†’ MinIO (Bronze Storage)
   â†’ Airflow DAG 
   â†’ Snowflake (Bronze â†’ Silver â†’ Gold via dbt)
   â†’ Power BI Analytics Dashboard
```

---

## âš¡ Tech Stack

### **Data Ingestion & Streaming**

* Python (API ingestion)
* Finnhub Stock Market API
* Apache Kafka (Producer + Consumer)

### **Object Storage**

* MinIO (S3 compatible)

### **Orchestration**

* Apache Airflow (managed in Docker)

### **Warehouse & Transformations**

* Snowflake Cloud Data Warehouse
* dbt Core (Bronze â†’ Silver â†’ Gold models)

### **Infrastructure**

* Docker & Docker Compose
* Postgres (Airflow backend)

### **Visualization**

* Power BI Dashboard connected to Snowflake

---

## ğŸ“Œ Key Features

* Real-time streaming of live stock prices from Finnhub API
* Kafka pipeline (Producer â†’ Broker â†’ Consumer)
* Storage of raw JSON into MinIO (Bronze layer)
* Automated ingestion to Snowflake via Airflow
* dbt transformations for clean Silver & analytical Gold layer
* Power BI dashboards built directly on Snowflake Gold models
* Fully containerized using Docker Compose

âš ï¸ **Note:**
In this project run, data was successfully streamed for:

* **AAPL**
* **GOOGL**
* **AMZN**

TSLA and MSFT were configured but **not streamed during ingestion time** and therefore do not appear in downstream layers.

---

## ğŸ“‚ Repository Structure

*(Matches Jayâ€™s structure exactly as you requested)*

```
real-time-stocks-mds/
â”‚
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ docker-compose.yml              # Kafka, Zookeeper, Airflow, MinIO, Postgres
â”‚   â”œâ”€â”€ producer/
â”‚   â”‚   â””â”€â”€ producer.py                 # Fetches API data â†’ Kafka
â”‚   â”œâ”€â”€ consumer/
â”‚   â”‚   â””â”€â”€ consumer.py                 # Kafka consumer â†’ MinIO
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ minio_to_snowflake.py       # Airflow DAG for Snowflake ingestion
â”‚
â”œâ”€â”€ dbt_stocks/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”‚   â”œâ”€â”€ bronze_stg_stock_quotes.sql
â”‚   â”‚   â”‚   â””â”€â”€ sources.yml
â”‚   â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”‚   â””â”€â”€ silver_clean_stock_quotes.sql
â”‚   â”‚   â””â”€â”€ gold/
â”‚   â”‚       â”œâ”€â”€ gold_candlestick.sql
â”‚   â”‚       â”œâ”€â”€ gold_kpi.sql
â”‚   â”‚       â””â”€â”€ gold_treechart.sql
â”‚   â”œâ”€â”€ macros/
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ powerbi/
â”‚   â””â”€â”€ stock_dashboard.pbix
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Implementation Steps

### **1. Kafka Setup (Docker)**

* `docker-compose.yml` initializes Kafka, Zookeeper, Airflow, MinIO, Postgres.
* Kafka topic created: `stocks-quotes`

---

### **2. Live Market Producer (Python)**

* Uses Finnhub API to fetch real-time stock prices.
* Streams JSON records into Kafka every 6 seconds.
* Symbols used: AAPL, GOOGL, AMZN (TSLA & MSFT not streamed this time).

---

### **3. Kafka Consumer â†’ MinIO (Bronze Layer)**

* Consumes messages from Kafka
* Saves each message as a JSON file to MinIO:

```
s3://bronze-transactions/<symbol>/<timestamp>.json
```

---

### **4. Airflow Orchestration**

DAG: `minio_to_snowflake.py`
Runs every 1 minute to:

1. Download raw JSON files from MinIO
2. Upload them to Snowflake internal stage
3. Run a `COPY INTO` into Snowflake Bronze table

---

### **5. Snowflake Setup**

Created:

* Warehouse: `COMPUTE_WH`
* Database: `STOCKS_MDS`
* Schema: `COMMON`
* Table: `BRONZE_STOCK_QUOTES_RAW`

Contains raw JSON data from MinIO.

---

### **6. dbt Transformations**

#### **Bronze â†’ Silver**

Bronze:

* Flatten raw JSON fields
* Cast datatypes
* Standardize naming

Silver:

* Clean up nulls
* Fix timestamp formatting
* Calculate derived metrics

#### **Gold Models**

* **gold_kpi** â€” key price changes & percent movement
* **gold_candlestick** â€” OHLC candlestick chart data
* **gold_treechart** â€” aggregated stock trend view

---

## ğŸ“Š Power BI Dashboard

Connected directly to Snowflake Gold layer via DirectQuery.

Includes:

* Candlestick OHLC chart
* Tree map of price trends
* KPIs & metrics
* Real-time comparison visuals for AAPL, AMZN, GOOGL

---

## ğŸ Final Outcomes

âœ” Fully functional real-time data pipeline
âœ” Raw â†’ Bronze â†’ Silver â†’ Gold modeling complete
âœ” Automated Airflow ingestion
âœ” dbt transformations executed successfully
âœ” Analytics dashboard built in Power BI
âœ” Professional, portfolio-ready data engineering project

---

## ğŸ‘©â€ğŸ’» Author

**Richa Desai**
USC MSBA | Data Engineering | Analytics | Cloud
*www.linkedin.com/in/richadesaiusc*


